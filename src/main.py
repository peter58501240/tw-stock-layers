from __future__ import annotations

BACKFILL_DAYS = 10  # å¾€å‰è£œ 10 å€‹äº¤æ˜“æ—¥

import os
import sys
import datetime as dt
from dataclasses import dataclass
from typing import Optional, Tuple, Any, List

import numpy as np
import pandas as pd
import requests

# -----------------------------
# Config
# -----------------------------
TWSE_BASE = "https://openapi.twse.com.tw/v1"
TPEX_BASE = "https://www.tpex.org.tw/openapi/v1"  # å¯èƒ½éœ€è¦å†èª¿ï¼Œä½†æœ¬ç‰ˆå·²å®¹éŒ¯

# ç«¯é»
TWSE_DAILY_ALL = f"{TWSE_BASE}/exchangeReport/STOCK_DAY_ALL"

# TPEx ç«¯é»å€™é¸ï¼ˆä»»ä¸€æˆåŠŸå³å¯ï¼‰
TPEX_DAILY_ALL_CANDIDATES = [
    f"{TPEX_BASE}/stock_aftertrading_daily_trading_info",
    f"{TPEX_BASE}/tpex_mainboard_daily",
]

HISTORY_PATH = "outputs/history_prices.csv"
OUT_HTML = "docs/index.html"

# v7.9.9.1 MVP åƒæ•¸ï¼ˆæŠ€è¡“é¢å­é›†ï¼‰
E_DRAWDOWN = 0.08      # -8%
MIN_HISTORY_FOR_MA20 = 20
MIN_HISTORY_FOR_MA60 = 60
MIN_HISTORY_FOR_MA240 = 240

# æ¬„ä½åˆ¥åå°ç…§ï¼šä¸åŒè³‡æ–™æºå‘½åä¸åŒï¼Œä¸€å¾‹è½‰æˆçµ±ä¸€æ¬„ä½
COLUMN_ALIASES = {
    # æ—¥æœŸ
    "Date": "date",
    "æ—¥æœŸ": "date",
    # ä»£è™Ÿ/åç¨±
    "code": "code",
    "Code": "code",
    "StockNo": "code",
    "è­‰åˆ¸ä»£è™Ÿ": "code",
    "name": "name",
    "Name": "name",
    "è­‰åˆ¸åç¨±": "name",
    # æˆäº¤é‡
    "volume": "volume",
    "Volume": "volume",
    "æˆäº¤é‡": "volume",
    "æˆäº¤è‚¡æ•¸": "volume",
    # æ”¶ç›¤åƒ¹
    "close": "close",
    "Close": "close",
    "ClosingPrice": "close",
    "æ”¶ç›¤åƒ¹": "close",
    "æ”¶ç›¤": "close",
    # é–‹é«˜ä½ï¼ˆå‚™ç”¨ï¼‰
    "OpeningPrice": "open",
    "HighestPrice": "high",
    "LowestPrice": "low",
    # æˆäº¤é‡‘é¡ï¼ˆå‚™ç”¨ï¼‰
    "TradeValue": "trade_value",
    # æ¼²è·Œï¼ˆå‚™ç”¨ï¼‰
    "Change": "change",
    # æˆäº¤ç­†æ•¸ï¼ˆå‚™ç”¨ï¼‰
    "Transaction": "transactions",
}


@dataclass
class FetchResult:
    df: pd.DataFrame
    source: str
    ok: bool
    error: Optional[str] = None
    warn: Optional[str] = None


def _http_get(url: str, timeout: int = 30) -> Tuple[int, str, str]:
    """
    å›å‚³ (status_code, content_type, text)
    """
    r = requests.get(url, timeout=timeout, headers={"User-Agent": "tw-stock-layers/1.1"})
    ct = r.headers.get("content-type", "")
    return r.status_code, ct, r.text


def _try_parse_json(text: str) -> Tuple[bool, Optional[Any], Optional[str]]:
    try:
        return True, requests.models.complexjson.loads(text), None
    except Exception as e:
        return False, None, f"{type(e).__name__}: {e}"


def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    å°‡ä¸åŒä¾†æºæ¬„ä½åç¨±çµ±ä¸€æˆï¼šdate/code/name/close/volume/...
    """
    if df is None or df.empty:
        return df

    new_cols = {}
    for c in df.columns:
        if c in COLUMN_ALIASES:
            new_cols[c] = COLUMN_ALIASES[c]
        else:
            # ä¹Ÿå˜—è©¦ä»¥ lower å»å°æ‡‰
            lc = str(c).strip()
            if lc in COLUMN_ALIASES:
                new_cols[c] = COLUMN_ALIASES[lc]
            else:
                new_cols[c] = c  # ä¿ç•™åŸæ¬„ä½ï¼ˆä¸å½±éŸ¿æ ¸å¿ƒï¼‰
    df = df.rename(columns=new_cols)

    # code ä¸€å¾‹å­—ä¸²
    if "code" in df.columns:
        df["code"] = df["code"].astype(str).str.strip()

    return df


def fetch_twse_daily_all(for_date: Optional[pd.Timestamp] = None) -> FetchResult:
    """
    å–å¾— TWSE å…¨å¸‚å ´æ—¥è³‡æ–™ã€‚
    è‹¥ for_date æœ‰å€¼ï¼Œå˜—è©¦ç”¨ querystring å¸¶å…¥æ—¥æœŸï¼ˆYYYYMMDDï¼‰ã€‚
    å–ä¸åˆ°å°±å› ok=False ä½†ä¸ä¸Ÿä¾‹å¤–ã€‚
    """
    try:
        url = TWSE_DAILY_ALL
        if for_date is not None:
            ymd = for_date.strftime("%Y%m%d")
            # å˜—è©¦å¸¸è¦‹åƒæ•¸åç¨±ï¼šdate
            url = f"{TWSE_DAILY_ALL}?date={ymd}"

        status, ct, text = _http_get(url)
        if status != 200:
            return FetchResult(pd.DataFrame(), "TWSE", False, f"HTTP {status} from TWSE: {url}")

        ok, data, jerr = _try_parse_json(text)
        if not ok or data is None:
            return FetchResult(pd.DataFrame(), "TWSE", False, f"TWSE JSON parse failed: {jerr}")

        df = pd.DataFrame(data)
        df = normalize_columns(df)

        if not {"code", "close"}.issubset(df.columns):
            return FetchResult(df, "TWSE", False, f"TWSE missing required columns after normalize: {list(df.columns)[:30]}")

        df["market"] = "TWSE"
        return FetchResult(df, "TWSE", True)

    except Exception as e:
        return FetchResult(pd.DataFrame(), "TWSE", False, f"{type(e).__name__}: {e}")

    def backfill_twse_recent_days(history: pd.DataFrame, today: pd.Timestamp, days: int) -> Tuple[pd.DataFrame, list[str]]:
    """
    å¾€å‰å›è£œæœ€è¿‘ days å€‹ã€Œäº¤æ˜“æ—¥ã€çš„è³‡æ–™ï¼ˆä»¥æ—¥ç‚ºæ­¥é€²ï¼ŒæŠ“ä¸åˆ°å°±è·³éï¼‰ã€‚
    åªå›è£œ TWSEï¼ˆTPEx å…ˆä¸å¼·æ±‚ï¼‰ã€‚
    """
    notes = []
    if days <= 0:
        return history, notes

    # å·²ç¶“æœ‰è³‡æ–™çš„æ—¥æœŸé›†åˆï¼ˆTWSEï¼‰
    have_dates = set(
        pd.to_datetime(history.loc[history["market"] == "TWSE", "date"]).dt.date.astype(str).tolist()
    ) if not history.empty else set()

    filled = 0
    # å¾€å‰æœ€å¤šæƒ 2*days å¤©ï¼ˆé¿å…é‡åˆ°é€£å‡å®Œå…¨è£œä¸åˆ°ï¼‰
    for i in range(1, days * 2 + 1):
        d = today - pd.Timedelta(days=i)
        d_key = d.date().isoformat()
        if d_key in have_dates:
            continue

        r = fetch_twse_daily_all(d)
        if not r.ok or r.df.empty:
            continue

        history = append_today(history, r.df, d)
        have_dates.add(d_key)
        filled += 1
        if filled >= days:
            break

    if filled > 0:
        notes.append(f"ğŸŸ¢ TWSE å·²å›è£œè¿‘ {filled} å€‹äº¤æ˜“æ—¥ï¼ˆç›®æ¨™ {days}ï¼‰")
    else:
        notes.append("ğŸŸ  TWSE å›è£œå¤±æ•—ï¼šå¯èƒ½ç«¯é»ä¸æ”¯æ´ date åƒæ•¸æˆ–è¢«é™åˆ¶ï¼ˆä»å¯æ¯æ—¥ç´¯ç©ï¼‰")

    return history, notes


def fetch_tpex_daily_all() -> FetchResult:
    last_warn = None
    last_err = None

    for url in TPEX_DAILY_ALL_CANDIDATES:
        try:
            status, ct, text = _http_get(url)
            if status != 200:
                last_err = f"TPEX HTTP {status}: {url}"
                continue

            # æœ‰äº›æ™‚å€™æœƒå› HTML æˆ–ç©ºå­—ä¸²
            if text is None or len(text.strip()) == 0:
                last_warn = f"TPEX empty response (likely blocked or no data): {url}"
                continue

            # å¦‚æœ content-type ä¸æ˜¯ jsonï¼Œä¹Ÿå…ˆå˜—è©¦ parseï¼›å¤±æ•—å°±ç•¶å®¹éŒ¯ç•¥é
            ok, data, jerr = _try_parse_json(text)
            if not ok or data is None:
                # å®¹éŒ¯ï¼šè¨˜ warnï¼Œä¸ä¸­æ–·
                snippet = text.strip()[:120].replace("\n", " ")
                last_warn = f"TPEX non-JSON response: {url} ({jerr}); head='{snippet}'"
                continue

            df = pd.DataFrame(data)
            df = normalize_columns(df)

            if {"code", "close"}.issubset(df.columns):
                df["market"] = "TPEx"
                return FetchResult(df, "TPEx", True, warn=last_warn)
            else:
                last_warn = f"TPEX schema unexpected after normalize: {url}, columns={list(df.columns)[:30]}"
                continue

        except Exception as e:
            last_err = f"TPEX exception: {type(e).__name__}: {e}"
            continue

    # é€™è£¡æ”¹æˆã€Œok=False ä½†ä¸è‡´å‘½ã€ï¼šä¸»æµç¨‹æœƒç”¨ warning å‘ˆç¾ä¸¦ç¹¼çºŒè·‘ TWSE
    return FetchResult(pd.DataFrame(), "TPEx", False, error=last_err, warn=last_warn)


def load_history() -> pd.DataFrame:
    if os.path.exists(HISTORY_PATH):
        try:
            hist = pd.read_csv(HISTORY_PATH, dtype={"code": str})
            hist["date"] = pd.to_datetime(hist["date"])
            return hist
        except Exception:
            pass
    return pd.DataFrame(columns=["date", "code", "market", "close", "volume"])


def append_today(history: pd.DataFrame, today_df: pd.DataFrame, today: pd.Timestamp) -> pd.DataFrame:
    keep = ["code", "market", "close"]
    if "volume" in today_df.columns:
        keep.append("volume")

    df = today_df[keep].copy()
    df["date"] = today
    df["code"] = df["code"].astype(str).str.strip()

    out = pd.concat([history, df], ignore_index=True)
    out["date"] = pd.to_datetime(out["date"])
    out = out.drop_duplicates(subset=["date", "code", "market"], keep="last")
    return out


def compute_indicators(hist: pd.DataFrame) -> pd.DataFrame:
    hist = hist.sort_values(["market", "code", "date"]).copy()
    hist["close"] = pd.to_numeric(hist["close"], errors="coerce")
    if "volume" in hist.columns:
        hist["volume"] = pd.to_numeric(hist["volume"], errors="coerce")
    else:
        hist["volume"] = np.nan

    def _grp(g: pd.DataFrame) -> pd.DataFrame:
        g = g.copy()
        g["ma20"] = g["close"].rolling(MIN_HISTORY_FOR_MA20).mean()
        g["ma60"] = g["close"].rolling(MIN_HISTORY_FOR_MA60).mean()
        g["ma240"] = g["close"].rolling(MIN_HISTORY_FOR_MA240).mean()
        g["hi_close"] = g["close"].cummax()
        g["dd_from_hi"] = g["close"] / g["hi_close"] - 1.0
        return g

    return hist.groupby(["market", "code"], group_keys=False).apply(_grp)


def layer_logic(today_row: pd.Series) -> Tuple[str, str]:
    """
    MVP åˆ†å±¤ï¼ˆæŠ€è¡“é¢å­é›†ï¼‰ï¼š
    - MA ä¸è¶³ï¼šZï¼ˆä¾ Â§11ï¼šç¼ºå€¼ä¸æ”¾å¯¬ï¼‰
    - è¿‘ä¼¼ Eï¼šæ”¶ç›¤ > MA60 ä¸” > MA20 ä¸”æœªå›è½ -8%
    - B/C/Dï¼šä»¥ MA240/MA60 ç²—åˆ†
    - Aï¼šæ­¤ MVP å…ˆä¿ç•™ç©ºï¼ˆå¾…è£œ RS/ç‡Ÿæ”¶/é‡èƒ½æ¢ä»¶å¾Œå†é–‹ï¼‰
    """
    if pd.isna(today_row.get("ma60")) or pd.isna(today_row.get("ma240")) or pd.isna(today_row.get("ma20")):
        return "Z", "Â§11 è³‡æ–™ä¸è¶³ï¼šMA ä¸è¶³ï¼Œåˆ—è§€å¯Ÿ"

    close = float(today_row["close"])
    ma20 = float(today_row["ma20"])
    ma60 = float(today_row["ma60"])
    ma240 = float(today_row["ma240"])
    dd = float(today_row.get("dd_from_hi", 0.0))

    if close > ma60 and close > ma20 and dd > -E_DRAWDOWN:
        return "E", "è¿‘ä¼¼Eï¼šæ”¶ç›¤>MA60ä¸”>MA20ä¸”æœªå›è½-8%"

    if close > ma240 and close > ma60:
        return "B", "è¶¨å‹¢ï¼šæ”¶ç›¤>MA240ä¸”>MA60ï¼ˆMVPï¼‰"
    if close > ma240 and close <= ma60:
        return "C", "å›æª”ï¼šæ”¶ç›¤>MA240ä½†â‰¤MA60ï¼ˆMVPï¼‰"
    if close <= ma240 and close > ma60:
        return "D", "åå½ˆï¼šæ”¶ç›¤â‰¤MA240ä½†>MA60ï¼ˆMVPï¼‰"

    return "Z", "å¼±å‹¢ï¼šæœªé”è¶¨å‹¢æ¢ä»¶ï¼Œåˆ—è§€å¯Ÿï¼ˆMVPï¼‰"


def build_html_report(date_str: str, layers: pd.DataFrame, warnings: List[str], errors: List[str]) -> str:
    def _table(df: pd.DataFrame, title: str) -> str:
        if df.empty:
            return f"<h2>{title}</h2><p>(ç©º)</p>"
        cols = ["market", "code", "name", "close", "layer", "reason"]
        df2 = df.copy()
        for c in cols:
            if c not in df2.columns:
                df2[c] = ""
        df2 = df2[cols]
        return f"<h2>{title}</h2>" + df2.to_html(index=False, escape=True)

    html = []
    html.append("<!doctype html><html><head><meta charset='utf-8'>")
    html.append("<meta name='viewport' content='width=device-width, initial-scale=1'>")
    html.append("<title>TW Stock Layers - Daily</title>")
    html.append("<style>body{font-family:system-ui,-apple-system,Segoe UI,Roboto,Arial; margin:24px;} table{border-collapse:collapse; width:100%;} th,td{border:1px solid #ddd; padding:8px;} th{background:#f5f5f5;}</style>")
    html.append("</head><body>")
    html.append(f"<h1>æ¯æ—¥åˆ†å±¤å ±è¡¨</h1><p>æ—¥æœŸï¼š{date_str}ï¼ˆAsia/Taipeiï¼‰</p>")

    if errors:
        html.append("<h2>æŠ“å–éŒ¯èª¤</h2><ul>")
        for e in errors:
            html.append(f"<li>{e}</li>")
        html.append("</ul>")

    if warnings:
        html.append("<h2>å³æ™‚è­¦ç¤ºï¼ˆMVPï¼‰</h2><ul>")
        for w in warnings:
            html.append(f"<li>{w}</li>")
        html.append("</ul>")

    for layer in ["A", "B", "C", "D", "E", "Z"]:
        df_layer = layers[layers["layer"] == layer].sort_values(["market", "code"])
        html.append(_table(df_layer, f"{layer} å±¤"))

    html.append("<hr><p style='color:#666'>è¨»ï¼šæœ¬ç‰ˆæœ¬ç‚ºå¯ä¸Šç·š MVPã€‚MA/RS/ç‡Ÿæ”¶/é‡èƒ½ç­‰è³‡æ–™è£œé½Šå¾Œï¼Œåˆ†å±¤å°‡é€æ­¥è²¼è¿‘ v7.9.9.1 å…¨æ¢æ–‡ã€‚</p>")
    html.append("</body></html>")
    return "\n".join(html)


def main() -> int:
    os.makedirs("docs", exist_ok=True)
    os.makedirs("outputs", exist_ok=True)

    today = pd.Timestamp(dt.datetime.now().date())
    date_str = today.strftime("%Y-%m-%d")

    errors: List[str] = []
    warnings: List[str] = []

    twse = fetch_twse_daily_all()
    if not twse.ok:
        errors.append(f"TWSE å–å¾—å¤±æ•—ï¼š{twse.error}")

    tpex = fetch_tpex_daily_all()
    if not tpex.ok:
        # TPEx ä¸è‡´å‘½ï¼šæ”¹æˆ warningï¼ˆä¸è®“æ•´å€‹æµç¨‹æ›æ‰ï¼‰
        if tpex.warn:
            warnings.append(f"ğŸŸ  TPEx å–å¾—ç•°å¸¸ï¼š{tpex.warn}")
        if tpex.error:
            warnings.append(f"ğŸŸ  TPEx ä¾‹å¤–ï¼š{tpex.error}")
    else:
        if tpex.warn:
            warnings.append(f"ğŸŸ¡ TPEx æç¤ºï¼š{tpex.warn}")

    # åˆä½µè³‡æ–™ï¼ˆåªè¦å…¶ä¸­ä¸€å€‹æœ‰è³‡æ–™å°±ç¹¼çºŒï¼‰
    frames = []
    if twse.ok and not twse.df.empty:
        frames.append(twse.df)
    if tpex.ok and not tpex.df.empty:
        frames.append(tpex.df)

    daily = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()
    if daily.empty:
        if not errors:
            errors.append("ä»Šæ—¥è³‡æ–™ç‚ºç©ºï¼ˆå¯èƒ½å…©å¸‚å ´è³‡æ–™æºçš†æš«æ™‚ä¸å¯ç”¨ï¼‰")
        html = build_html_report(date_str, pd.DataFrame(columns=["market", "code", "name", "close", "layer", "reason"]), warnings, errors)
        with open(OUT_HTML, "w", encoding="utf-8") as f:
            f.write(html)
        return 0

    # è®€å–/ç´¯ç©æ­·å²
    hist = load_history()

# è‹¥æ­·å²ä¸è¶³ï¼Œå…ˆå›è£œæœ€è¿‘ N å€‹äº¤æ˜“æ—¥ï¼ˆTWSEï¼‰
hist, bf_notes = backfill_twse_recent_days(hist, today, BACKFILL_DAYS)
warnings.extend(bf_notes)

# å†æŠŠä»Šå¤©è³‡æ–™å¯«å…¥
hist = append_today(hist, daily, today)
hist.to_csv(HISTORY_PATH, index=False, encoding="utf-8")


    # è¨ˆç®—æŒ‡æ¨™
    hist_ind = compute_indicators(hist)
    today_ind = hist_ind[hist_ind["date"] == today].copy()
    if today_ind.empty:
        errors.append("ä»Šæ—¥æŒ‡æ¨™è³‡æ–™ç‚ºç©ºï¼ˆå¯èƒ½æ˜¯æ—¥æœŸæ ¼å¼æˆ–å¯«å…¥å¤±æ•—ï¼‰")

    # åˆ†å±¤
    out_rows = []
    for _, row in today_ind.iterrows():
        layer, reason = layer_logic(row)
        out_rows.append({
            "market": row.get("market", ""),
            "code": str(row.get("code", "")).strip(),
            "name": row.get("name", ""),
            "close": row.get("close", ""),
            "layer": layer,
            "reason": reason,
        })
    layers = pd.DataFrame(out_rows)

    # MVP è­¦ç¤º
    z_ratio = (layers["layer"] == "Z").mean() if len(layers) else 1.0
    if z_ratio > 0.8:
        warnings.append("ğŸŸ¡ Z å±¤å æ¯”åé«˜ï¼šæ­·å²è³‡æ–™å°šåœ¨ç´¯ç©ï¼ˆç¬¦åˆ Â§11 ç¼ºå€¼é™ç´šï¼‰")

    if twse.ok and tpex.ok:
        warnings.append("ğŸŸ¢ TWSE/TPEx çš†å·²å–å¾—ï¼ˆè‹¥ TPEx ç‚ºç©ºå±¬æ­£å¸¸æ™‚æ®µå·®ç•°ï¼‰")
    elif twse.ok and not tpex.ok:
        warnings.append("ğŸŸ  ä»Šæ—¥åƒ… TWSE å¯ç”¨ï¼šTPEx ä¾ Â§11 é™ç´šè™•ç†")
    elif (not twse.ok) and tpex.ok:
        warnings.append("ğŸŸ  ä»Šæ—¥åƒ… TPEx å¯ç”¨ï¼šTWSE ä¾ Â§11 é™ç´šè™•ç†")

    # ç”¢å‡º HTML
    html = build_html_report(date_str, layers, warnings, errors)
    with open(OUT_HTML, "w", encoding="utf-8") as f:
        f.write(html)

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
